# ğŸš— Car Rental Data Processing Pipeline - Project Documentation

## ğŸ“Œ Project Overview

This project implements a scalable and automated **Big Data Processing Pipeline** for a **Car Rental Marketplace** using AWS services. It leverages **Apache Spark on EMR** to process raw CSV data, **AWS Glue** for cataloging, **Amazon Athena** for querying, and **Step Functions** for orchestration.

---

## ğŸ§± Architecture

### âœ… AWS Services Used

* **Amazon S3**: Raw and processed data storage
* **Amazon EMR**: Spark cluster for transformation jobs
* **AWS Glue**: Crawlers to catalog Parquet outputs
* **Amazon Athena**: SQL querying engine
* **AWS Step Functions**: Workflow orchestration

---

## ğŸ“ Project Structure

```
car-rental-pipeline/
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ job1_location_metrics.py
â”‚   â”œâ”€â”€ job2_user_metrics.py
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â”œâ”€â”€ rental_transactions.csv
â”‚   â”‚   â”œâ”€â”€ users.csv
â”‚   â”‚   â”œâ”€â”€ vehicles.csv
â”‚   â”‚   â””â”€â”€ locations.csv
â”‚   â””â”€â”€ processed/
â”‚
â”œâ”€â”€ stepfunctions/
â”‚   â”œâ”€â”€ emr_workflow_with_athena.json
â”‚   â”œâ”€â”€ emr_workflow_basic.json
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ glue_crawlers/
â”‚   â”œâ”€â”€ crawler_definitions.md
â”‚   â””â”€â”€ glue_catalog_structure.md
â”‚
â”œâ”€â”€ athena_queries/
â”‚   â”œâ”€â”€ top_users_by_spend.sql
â”‚   â”œâ”€â”€ daily_revenue_trend.sql
â”‚   â”œâ”€â”€ top_locations_by_revenue.sql
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ architecture_diagram.drawio
â”‚   â”œâ”€â”€ architecture_diagram.png
â”‚   â”œâ”€â”€ project_documentation.md
â”‚   â””â”€â”€ IAM_permissions.md
â”‚
â”œâ”€â”€ cloudformation/
â”‚   â”œâ”€â”€ emr_cluster.yaml
â”‚   â”œâ”€â”€ glue_crawlers.yaml
â”‚   â””â”€â”€ step_functions.yaml
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore
```

---

## ğŸ”¥ PySpark Jobs

### 1ï¸âƒ£ job1\_location\_metrics.py

* Computes location-based and vehicle-based KPIs
* Output: `processed/location_metrics/`, `processed/vehicle_metrics/`

### 2ï¸âƒ£ job2\_user\_metrics.py

* Computes user KPIs and daily trends
* Output: `processed/user_transaction_metrics/user_metrics/`, `processed/user_transaction_metrics/daily_metrics/`

---

## ğŸ§  Glue Crawlers

### Database: `car_rental_db`

| Crawler Name               | S3 Target Path                                      | Table Name         |
| -------------------------- | --------------------------------------------------- | ------------------ |
| `crawler_location_metrics` | `processed/location_metrics/`                       | `location_metrics` |
| `crawler_vehicle_metrics`  | `processed/vehicle_metrics/`                        | `vehicle_metrics`  |
| `crawler_user_metrics`     | `processed/user_transaction_metrics/user_metrics/`  | `user_metrics`     |
| `crawler_daily_metrics`    | `processed/user_transaction_metrics/daily_metrics/` | `daily_metrics`    |

---

## ğŸ” Athena Setup

1. Set Query Result Location: `s3://car-rental-data-project/athena-results/`
2. Database: `car_rental_db`
3. Sample Queries:

```sql
SELECT user_id, total_spent FROM user_metrics ORDER BY total_spent DESC LIMIT 5;
SELECT rental_date, daily_revenue FROM daily_metrics ORDER BY rental_date;
SELECT pickup_location, total_revenue FROM location_metrics ORDER BY total_revenue DESC LIMIT 5;
```

---

## ğŸ”„ Step Functions Automation

### Workflow:

1. Run EMR Spark jobs
2. Trigger Glue crawlers
3. Run Athena queries in parallel

### IAM Requirements:

* `elasticmapreduce:AddStep`
* `glue:StartCrawler`
* `athena:StartQueryExecution`, `athena:GetQueryExecution`, `athena:GetQueryResults`
* `s3:PutObject` and `s3:GetObject` to `athena-results/`

---

## ğŸ§¾ requirements.txt

```txt
pyspark==3.3.0
boto3
```

---

## ğŸ“˜ README.md

```markdown
# ğŸš— Car Rental Analytics Pipeline

A fully automated big data pipeline built with PySpark on EMR, Glue, Athena, and Step Functions.

## ğŸ“Š Features
- Process raw rental data in S3 using Spark
- Store KPIs as Parquet in `processed/`
- Catalog datasets using AWS Glue Crawlers
- Query KPIs using Amazon Athena
- Orchestrated via AWS Step Functions

## ğŸ—‚ Project Structure
Refer to `docs/project_documentation.md` or see this summary:
- `scripts/`: PySpark jobs
- `data/raw/`: Raw CSV input
- `athena_queries/`: SQL analytics
- `stepfunctions/`: Automation definitions

## ğŸš€ Getting Started
1. Upload raw data to `s3://car-rental-data-project/raw/`
2. Upload PySpark scripts to `s3://car-rental-data-project/scripts/`
3. Run Step Function (`emr_workflow_with_athena.json`)
4. Check Athena `car_rental_db` for output

## ğŸ“ˆ Example Queries
- Top 5 Users by Spend
- Daily Revenue Trend
- Top Locations by Revenue

## ğŸ›¡ IAM Policy Requirements
See `docs/IAM_permissions.md`

## ğŸ“Œ Notes
- EMR cluster must be configured with Spark
- Glue Crawlers must be run after Spark jobs complete
```

---

Let me know if you'd like me to generate this as downloadable files, ZIP package, or GitHub-ready folder. âœ…
